---
title: "Static linear panel data models"
subtitle: "Tutorial 2"
date: "Stanislav Avdeev"
output:
  xaringan::moon_reader:
    self_contained: TRUE
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE) 
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, dpi = 200, fig.width = 8, fig.height = 5)
library(tidyverse)
library(gganimate)
library(estimatr)
library(magick)
library(dagitty)
library(ggthemes)
library(directlabels)
library(ggdag)
library(jtools)
library(scales)
library(Cairo)
library(modelsummary)
library(stargazer)
library(wooldridge)
library(transformr)
library(huxtable)

theme_metro <- function(x) {
  theme_classic() + 
  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        text = element_text(size = 16),
        axis.title.x = element_text(hjust = 1),
        axis.title.y = element_text(hjust = 1, angle = 0))
}
theme_void_metro <- function(x) {
  theme_void() + 
  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        text = element_text(size = 16))
}
theme_metro_regtitle <- function(x) {
  theme_classic() + 
  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        text = element_text(size = 16))
}
```

# Goal for today's tutorial

1. Understand the panel structure of the data
1. Explore differences between pooled OLS, fixed, and random effects estimators
1. Interpret the variation in the data
1. Make proper inferences using panel data models

---

# Panel data

- Panel data contain information on the same individual over multiple time periods
  - "individual" could be a person, a company, a state, a country, etc. There are $N$ individuals
  - "time period" could be a year, a month, a day, etc. There are $T$ time periods
- We assume that we observe each individual the same number of times, i.e. a **balanced** panel (so we have $N\times T$ observations)
  - you can use panel data estimators with unbalanced panels too, it just gets a little more complex

---

# Panel data

- Let's use a data set from `wooldridge` package on crime data 
  - you can use a lot of data sets from packages, such as `wooldridge` which contains data sets from "Introductory Econometrics: A Modern Approach" by Wooldridge J.M.
- Here's what a panel data set looks like - a variable for individual (county), a variable for time (year), and then the different variables

```{r, echo = FALSE}
data(crime4, package = 'wooldridge')
crime4 %>%
  select(county, year, crmrte, prbarr) %>%
  rename(County = county,
         Year = year,
         CrimeRate = crmrte,
         ProbofArrest = prbarr) %>%
  slice(c(1:4, 8:11)) %>%
  knitr::kable()
```

---

# Between and within variation

Let's pick a few counties and graph this out

```{r, echo = FALSE}
crime4 %>% 
  filter(county %in% c(1,3,7, 23),
         prbarr < .5) %>%
  group_by(county) %>%
  mutate(label = case_when(
    crmrte == max(crmrte) ~ paste('County',county),
    TRUE ~ NA_character_
  )) %>%
  ggplot(aes(x =  prbarr, y = crmrte, color = factor(county), label = label)) + 
  geom_point() + 
  geom_text(hjust = -.1, size = 14/.pt) + 
  theme_metro_regtitle() + 
  labs(x = 'Probability of Arrest', 
       y = 'Crime Rate') + 
  guides(color = FALSE, label = FALSE) + 
  scale_color_manual(values = c('black','blue','red','purple'))
```

---

# Between variation

If we look at the **between** variation by using the **pooled** OLS estimator, we get this

```{r, echo = FALSE}
crime4 %>% 
  filter(county %in% c(1,3,7, 23),
         prbarr < .5) %>%
  group_by(county) %>%
  mutate(label = case_when(
    crmrte == max(crmrte) ~ paste('County',county),
    TRUE ~ NA_character_
  )) %>%
  ggplot(aes(x =  prbarr, y = crmrte, color = factor(county), label = label)) + 
  geom_point() + 
  geom_text(hjust = -.1, size = 14/.pt) + 
  theme_metro_regtitle() + 
  labs(x = 'Probability of Arrest', 
       y = 'Crime Rate') + 
  guides(color = FALSE, label = FALSE) + 
  scale_color_manual(values = c('black','blue','red','purple')) + 
  geom_smooth(method = 'lm', aes(color = NULL, label = NULL), se = FALSE)
```

---

# Between variation

**Between** variation looks at the relationship **between the means of each county**

```{r, echo = FALSE}
crime4 %>% 
  filter(county %in% c(1,3,7, 23),
         prbarr < .5) %>%
  group_by(county) %>%
  mutate(label = case_when(
    crmrte == max(crmrte) ~ paste('County',county),
    TRUE ~ NA_character_
  ),
  mcrm = mean(crmrte),
  mpr = mean(prbarr)) %>%
  ggplot(aes(x =  prbarr, y = crmrte, color = factor(county), label = label)) + 
  geom_point() + 
  geom_text(hjust = -.1, size = 14/.pt) + 
  theme_metro_regtitle() + 
  labs(x = 'Probability of Arrest', 
       y = 'Crime Rate') + 
  guides(color = FALSE, label = FALSE) + 
  scale_color_manual(values = c('black','blue','red','purple')) + 
  geom_point(aes(x = mpr, y = mcrm), size = 20, shape = 3, color = 'darkorange') + 
  annotate(geom = 'text', x = .3, y = .02, label = 'Means Within Each County', color = 'darkorange', size = 14/.pt)
```

---

# Between variation

The individual year-to-year variation **within** county doesn't matter

```{r, echo = FALSE}
crime4 %>% 
  filter(county %in% c(1,3,7, 23),
         prbarr < .5) %>%
  group_by(county) %>%
  mutate(label = case_when(
    crmrte == max(crmrte) ~ paste('County',county),
    TRUE ~ NA_character_
  ),
  mcrm = mean(crmrte),
  mpr = mean(prbarr)) %>%
  ggplot(aes(x =  prbarr, y = crmrte, color = factor(county), label = label)) + 
  theme_metro_regtitle() + 
  labs(x = 'Probability of Arrest', 
       y = 'Crime Rate') + 
  guides(color = FALSE, label = FALSE) + 
  scale_color_manual(values = c('black','blue','red','purple')) + 
  geom_point(aes(x = mpr, y = mcrm), size = 20, shape = 3, color = 'darkorange') + 
  geom_smooth(aes(color = NULL), method = 'lm', se = FALSE)+
  annotate(geom = 'text', x = .3, y = .02, label = 'OLS Fit on These Four Points', color = 'blue', size = 14/.pt)
```

---

# Within variation

**Within** variation goes the other way: it looks at variation **within county from year-to-year**

```{r, echo = FALSE}
cranim <- crime4 %>% 
  filter(county %in% c(1,3,7, 23),
         prbarr < .5) %>%
  mutate(allcrm = mean(crmrte),
         allmpr = mean(prbarr)) %>%
  group_by(county) %>%
  mutate(label = case_when(
    crmrte == max(crmrte) ~ paste('County',county),
    TRUE ~ NA_character_
  ),
  mcrm = mean(crmrte),
  mpr = mean(prbarr),
  stage = '1. Raw Data')
cranim <- cranim %>%
  bind_rows(cranim %>% 
              mutate(crmrte = crmrte - mcrm + allcrm,
                     prbarr = prbarr - mpr + allmpr,
                     mcrm = allcrm,
                     mpr = allmpr,
                     stage = '2. Remove all between variation'))

p <- ggplot(cranim, aes(x =  prbarr, y = crmrte, color = factor(county), label = label)) + 
  geom_point() + 
  geom_text(hjust = -.1, size = 14/.pt)  + 
  labs(x = 'Probability of Arrest', 
       y = 'Crime Rate') + 
  guides(color = FALSE, label = FALSE) + 
  scale_color_manual(values = c('black','blue','red','purple')) + 
  geom_smooth(aes(color = NULL), method = 'lm', se = FALSE)+
  geom_point(aes(x = mpr, y = mcrm), size = 20, shape = 3, color = 'darkorange') + 
  transition_states(stage) + 
  theme_metro_regtitle()

animate(p, nframes = 80)
```

---

# Between and within variation

- We can clearly see that **between** counties there's a strong **positive** relationship
- But if you look **within** a given county, the relationship isn't that strong, and actually seems to be **negative**
  - which would make sense - if you think your chances of getting arrested are high, that should be a deterrent to crime
  - we are ignoring all differences between counties and looking only at differences within counties
- **Fixed effects** is sometimes also referred to as the **within estimator**

---

# Panel data model

- The $it$ subscript says this variable varies over individual $i$ and time $t$
$$Y_{it} = \alpha + X_{it}' \beta + U_{it}$$
- What if there are individual-level components in the error term causing omitted variable bias? 
  - $X_{it}$ might be related to the variable which is not in the model and thus in the error term
- Thus, we have the following model
$$Y_{it} = \alpha + X_{it}' \beta + \eta_i + U_{it}$$
- If you think $X_{it}$ and $\eta_i$ are not correlated (based on theory, previous research, tests), you can use both FE and RE estimators
- If you think $X_{it}$ and $\eta_i$ are correlated (based on theory, previous research, tests), use FE estimator

---

# Panel data model: simulation

- Let's simulate a panel data set

```{r, echo = TRUE}
set.seed(7)
df <- tibble(id = sort(rep(1:600, 10)), 
             time = rep(1:10, 600),
             x1 = rnorm(6000),
             # fixed variable within individual, e.g. gender
             x2 = ifelse(id %% 2 == 0, 0, 1),
             y = id + time + 2*x1 + 50*x2 + rnorm(6000))
```

```{r, echo = FALSE}
df %>%
  slice(c(1:3, 11:13)) %>%
  knitr::kable()
```

---

# Panel data model: simulation

```{r, echo = TRUE}
# The true effect is 2
library(plm) # package to estimate FE and RE models (fixest is preferred for FE)
pooled <- plm(y ~ x1 + x2, model = "pooling", df) # or lm(y ~ x1 + x2, df)
random <- plm(y ~ x1 + x2, model = "random", index = c("id", "time"), 
                           effect = "twoways", df)
fixed  <- plm(y ~ x1 + x2, model = "within", index = c("id", "time"), 
                           effect = "twoways", df)
```

```{r, echo = FALSE}
msummary(list(pooled, random, fixed), stars = TRUE, gof_omit = '^(?!Num)', coef_omit = "(Intercept)")
```

- Pooled OLS estimates are off as it doesn't take into account the panel structure of data
- RE and FE estimators provide **unbiased** estimates

---

# Panel data model: simulation

- Let's introduce the correlation between individual characteristics and individual effects

\begin{align*}
  \text{corr} (X_{it}, \eta_i) \neq 0
\end{align*}

```{r, echo = TRUE}
set.seed(7)
df <- tibble(id = sort(rep(1:600, 10)), 
             time = rep(1:10, 600),
             x1 = rnorm(6000) + 0.05*id, # add a correlated individual effect
             x2 = ifelse(id %% 2 == 0, 0, 1),
             y = id + time + 2*x1 + 50*x2 + rnorm(6000))
```

```{r, echo = FALSE}
df %>%
  slice(c(1:3, 11:13)) %>%
  knitr::kable()
```

---

# Panel data model: simulation

```{r, echo = TRUE}
# The true effect is 2
pooled_corr <- plm(y ~ x1 + x2, model = "pooling", df)
random_corr <- plm(y ~ x1 + x2, model = "random", index = c("id", "time"), 
                                effect = "twoways", df)
fixed_corr  <- plm(y ~ x1 + x2, model = "within", index = c("id", "time"), 
                                effect = "twoways", df)
```

```{r, echo = FALSE}
msummary(list(pooled_corr, random_corr, fixed_corr), stars = TRUE, gof_omit = '^(?!Num)', coef_omit = "(Intercept)")
```

- Pooled OLS and RE estimates are off since $\text{corr} (X_{it}, \eta_i) \neq 0$
- FE estimator still provides **unbiased** estimates since $\eta_i$ are eliminated

---

# Estimation: de-meaning approach

- To estimate FE model, we need to remove **between** variation so that all that's left is **within** variation
- There are two main ways that give the same results
  - **de-meaning**
  - **binary variables** 
- Let's do de-meaning first, since it's closely related to the "removing between variation" explanation
  - start with a standard panel data model
$$Y_{it} = \alpha + X_{it}' \beta + \eta_i + U_{it}$$
  - for each variable get the mean value of that variable for each individual
  - subtract out that mean to get residuals
$$Y_{it} - \bar{Y_i} = (\alpha - \alpha) + (X_{it} - \bar{X_i})'\beta + (\eta_i - \eta_i) + (U_{it} - \bar{U_i})$$
  - work with those residuals
$$Y_{it} - \bar{Y}_i = (X_{it} - \bar{X}_i)' \beta + (U_{it} - \bar{U_{i}})$$
- The residuals are, by construction, no longer related to the $\eta_i$

---

# Estimation: LSDV approach

- De-meaning the data is not the only way to do it
  - and sometimes it can make the standard errors wonky, since they don't recognize that you've estimated those means
- You can also use the **least squares dummy variable** - LSDV (another word for "binary variable") method
  - we just treat "individual" like the categorical variable and add it as a control

---

# Estimation: empirical example

- Let's get back to the crime data set
- To demean the data, we use `group_by()` to get means-within-groups and subtract them

```{r, echo = TRUE}
data(crime4, package = 'wooldridge')
crime4 <- crime4 %>%
  filter(county %in% c(1, 3, 7, 23), # filter to the data points from our graph
         prbarr < .5) %>%
  group_by(county) %>%
  mutate(mean_crime = mean(crmrte),
         mean_prob = mean(prbarr)) %>%
  mutate(demean_crime = crmrte - mean_crime,
         demean_prob = prbarr - mean_prob)
```

```{r, echo = FALSE}
crime4[c(1:2, 8:9),] %>%
  select(county, year, crmrte, prbarr, mean_crime, mean_prob, demean_crime, demean_prob) %>%
  knitr::kable()
```

---

# Estimation: empirical example

- To use least squares dummy variable, we only need to add FE as categorical variables

```{r, echo = TRUE}
pooling <- lm(crmrte ~ prbarr, data = crime4)
lsdv    <- lm(crmrte ~ prbarr + factor(county), data = crime4)
de_mean <- lm(demean_crime ~ demean_prob, data = crime4)
```

```{r, echo = FALSE}
msummary(list(pooling, lsdv, de_mean), stars = TRUE, gof_omit = '^(?!Num)', coef_map = c("prbarr", "demean_prob"))
```

---

# Interpreting a within relationship

- How can we interpret that slope of $`r round(lsdv[["coefficients"]][["prbarr"]], 3)`$?
  - this is all **within variation** so our interpretation must be **within a county**
  - if we think we've **causally** identified it then "raising the arrest probability by $1$ percentage point in a county reduces the number of crimes per person in that county by $-0.0003$"
  - we're basically **controlling for county**, i.e. comparing a county to itself at different points in time
- It’s possible to have more than one set of fixed effects
  - but interpretation gets tricky - think through what variation in $X$ you’re looking at

---

# Interpreting a within relationship

```{r, echo = FALSE}
crime4 %>%
  ungroup() %>%
  mutate(pred = predict(lsdv)) %>%
  group_by(county) %>%
  mutate(label = case_when(
    crmrte == max(crmrte) ~ paste('County',county),
    TRUE ~ NA_character_
  )) %>%
  ggplot(aes(x =  prbarr, y = crmrte, color = factor(county), label = label)) + 
  geom_point() + 
  geom_text(hjust = -.1, size = 14/.pt) + 
  geom_line(aes(y = pred, group = county), color = 'blue') +
  theme_metro_regtitle() + 
  labs(x = 'Probability of Arrest', 
       y = 'Crime Rate') + 
  guides(color = FALSE, label = FALSE) + 
  scale_color_manual(values = c('black','blue','red','purple'))
```

---

# Panel data: estimation

- Empirical researchers rarely do either of these, and rather will use a command specifically designed for the FE estimator
  - `feols()` in `fixest`
  - `felm()` in `lfe`
  - `plm()` in `plm`
  - `lm_robust()` in `estimatr`
- `feols()` in `fixest` seems to be a better choice
  - it does all sorts of other neat stuff like fixed effects in nonlinear models like logit, regression tables, joint-test functions, and so on 
  - it’s very fast, and can be easily adjusted to do fixed effects with other regression methods like logit, or combined with IV
  - it clusters the standard errors by the first fixed effect by default

---

# Panel data: estimation

Let's look at the output of `plm()` and `feols()`

```{r, echo = TRUE}
library(fixest)
fe_plm   <- plm(crmrte ~ prbarr, model = "within", index = "county", crime4)
fe_feols <- feols(crmrte ~ prbarr | county, crime4)
```

```{r, echo = FALSE}
msummary(list(fe_plm, fe_feols), stars = TRUE, gof_omit = '^(?!Num|Std)')
```

---

# Fixed effects: limitations

1. Fixed effects don't control for anything that has **within** variation
1. They control away everything that's **between** only, so we can't see the effect of anything that's between only (effect of geography on crime rate? nope)
1. Anything with only a **little within** variation will have most of its variation washed out too (effect of population density on crime rate? probably not)
1. If there’s not a lot of within variation, fixed effects are going to be very noisy. Make sure there’s variation to study
1. The FE estimator pays the most attention to individuals with **lots of variation in treatment**

- 2 and 3 can be addressed by using the RE estimator instead
  - although you need to be certain that 
$$\text{corr} (X_{it}, \eta_i) = 0$$
  - how can you check that?

---

# Fixed or random effects

- To decide between FE or RE estimators you can run the **Hausman test** where the null hypothesis is that the preferred model is the RE estimator vs. the alternative - the FE estimator
- The Hausman test is a broad set of tests that compare the estimates in one model against the estimates in another and sees if they are different
- It basically tests whether the errors are correlated with the regressors
  - under $H_0$: $\text{corr} (X_{it}, \eta_i) = 0$ and both RE and FE estimators are consistent, but the RE estimator is more efficient
  - under $H_1$: $\text{corr} (X_{it}, \eta_i) \neq 0$ and only FE estimator is consistent
- FE estimator is almost always preferred to the RE estimator, except when you are quite sure that the right-hand-side variables $X_{it}$ are unrelated to the individual effects $\eta_i$
---

# Fixed or random effects

- Let's apply it to two simulated data sets with and without correlated individual effects

```{r, echo = TRUE}
phtest(fixed, random)
phtest(fixed_corr, random_corr)
```

- As expected, we should use the RE estimator in the first model, and the FE estimator in the second model

---

# Panel data inference
- One of the assumptions of the regression model is that the error terms are independent of each other
  - however, we might imagine that some of the left variation is shared across all individuals, making them correlated with each other
  - thus, not taking that into account would make s.e. wrong
- Two conditions need to hold for clustering to be necessary
  - first, there needs to be **treatment effect heterogeneity**. That is, the treatment effect must be quite different for different individuals
- If that is true, there’s a second condition
  - either **DGP** is clustered, meaning the individuals/groups in your data represent a **non-random sampling of the population**. For example, some groups are more likely to be included in your sample than others
  - or **treatment assignment mechanism** is clustered, meaning within individuals/groups your **treatment variable is assigned in a clustered way**. For example, if you belong to a certain group, you are more likely to get treatment
- So before clustering, think about whether both conditions are likely to be true (Abadie et al. 2017)

---

# Panel data inference: simulation

```{r, echo = TRUE}
set.seed(7)
df <- tibble(id = sort(rep(1:600, 10)), 
             time = rep(1:10, 600),
             # we don't generate x2 as FE eliminates it anyway
             x1 = rnorm(6000), 
             # Now the error term has two components:
             # 1. the individual cluster (5*id), 
             # 2. the normal error term (rnorm(6000))
             y = id + time + 2*x1 + (5*id + rnorm(6000)))
```

```{r, echo = FALSE}
df %>%
  slice(c(1:3, 11:13)) %>%
  knitr::kable()
```

---

# Panel data inference: simulation

```{r, echo = TRUE}
# The true effect is 2
fe_clustered     <- feols(y ~ x1 | id, df) # we use only one set of fixed effects
fe_not_clustered <- feols(y ~ x1 | id, se = 'standard', df) # make s.e. i.i.d.
```

```{r, echo = FALSE}
msummary(list(fe_clustered, fe_not_clustered), stars = TRUE, gof_omit = '^(?!Num|Std)')
```

- It’s common to cluster s.e. at the level of the fixed effects, since it seems likely that errors would be correlated over time
  - `feols()` in `fixest` clusters by the first FE by default
- Not accounting for clustering at the individual level leads to incorrect s.e.

---

# References

Books
- Huntington-Klein, N. The Effect: An Introduction to Research Design and Causality, [Chapter 16: Fixed Effects](https://theeffectbook.net/ch-FixedEffects.html)
- Cunningham, S. Causal Inference: The Mixtape, [Chapter 8: Panel Data](https://mixtape.scunning.com/panel-data.html)

Slides
- Huntington-Klein, N. Econometrics Course, [Week 6: Within Variation and Fixed Effects](https://github.com/NickCH-K/EconometricsSlides/blob/master/Week_06/Week_06_1_Within_Variation_and_Fixed_Effects.html)
- Huntington-Klein, N. Causality Inference Course, [Lecture 8: Fixed Effects](https://github.com/NickCH-K/CausalitySlides/blob/main/Lecture_08_Fixed_Effects.html)

Articles
- Abadie, A., Athey, S., Imbens, G. W., & Wooldridge, J. (2017). [When Should You Adjust Standard Errors for Clustering?](https://www.nber.org/papers/w24003) (No. w24003). National Bureau of Economic Research
  