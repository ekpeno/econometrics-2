---
title: "Regression discontinuity design"
subtitle: "Tutorial 6"
date: "Stanislav Avdeev"
output:
  xaringan::moon_reader:
    self_contained: TRUE
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include = FALSE}
options(htmltools.dir.version = FALSE) 
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, dpi = 200, fig.width = 8, fig.height = 4.5)
library(tidyverse)
library(dagitty)
library(ggdag)
library(gganimate)
library(ggpubr)
library(ggthemes)
library(Cairo)
library(rdrobust)
library(modelsummary)
library(purrr)
library(AER)
library(estimatr)
library(magick)
library(directlabels)
library(fixest)
library(jtools)
library(scales)

theme_set(theme_gray(base_size = 15))
theme_metro <- function(x) {
  theme_classic() + 
  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        text = element_text(size = 16),
        axis.title.x = element_text(hjust = 1),
        axis.title.y = element_text(hjust = 1, angle = 0))
}
theme_void_metro <- function(x) {
  theme_void() + 
  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        text = element_text(size = 16))
}
theme_metro_regtitle <- function(x) {
  theme_classic() + 
  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        text = element_text(size = 16))
}
```

# Goal for today's tutorial

1. Discuss (fuzzy) regression discontinuity design - RDD
1. Discuss regression kink design - RKD
1. Discuss functional form, bandwidth, and controls in RDD
1. Check assumptions of RDD
1. Discuss sensitivity checks

---

# RDD

- The basic idea of RDD is to look for a treatment that is assigned on the basis of being above/below a **cutoff value** of a continuous variable, for example
  - if a candidate gets $50.1\%$ of the vote they're in, $40.9\%$ and they're out
  - if you're $65.1$ years old you get Medicaid, if you're $64.9$ years old you don't
  - if you score above $75$, you'll be admitted into a "gifted and talented" (GATE) program
- We call these continuous variables **running variables** because we run along them until we hit the cutoff
- Basically, the idea is that right around the cutoff, treatment is **randomly assigned**
  - if you have a test score of $74.9$ (not high enough for GATE), you're basically the same as someone who has a test score of $75.1$ (just barely high enough)
- So, we have two groups - the just-barely-missed-outs and the just-barely-made-its - that are basically exactly the same except that one happened to get treatment
  - this gives us the effect of treatment **for people who are right around the cutoff** - LATE

---

# RDD: graphically

```{r, echo=FALSE, fig.width = 5, fig.height = 5.5}
df <- data.frame(xaxisTime=runif(300)*20) %>%
  mutate(Y = .2*xaxisTime+3*(xaxisTime>10)-.1*xaxisTime*(xaxisTime>10)+rnorm(300),
         state="1",
         groupX=floor(xaxisTime)+.5,
         groupLine=floor(xaxisTime),
         cutLine=rep(c(9,11),150)) %>%
  group_by(groupX) %>%
  mutate(mean_Y=mean(Y)) %>%
  ungroup() %>%
  arrange(groupX)
dffull <- rbind(
  #Step 1: Raw data only
  df %>% mutate(groupLine=NA,cutLine=NA,mean_Y=NA,state='1. Start with raw data.'),
  #Step 2: Add Y-lines
  df %>% mutate(cutLine=NA,state='2. What differences in Y are explained by Running Variable?'),
  #Step 3: Collapse to means
  df %>% mutate(Y = mean_Y,state="3. Keep only what's explained by the Running Variable."),
  #Step 4: Zoom in on just the cutoff
  df %>% mutate(mean_Y = ifelse(xaxisTime > 9 & xaxisTime < 11,mean_Y,NA),Y=ifelse(xaxisTime > 9 & xaxisTime < 11,mean_Y,NA),groupLine=NA,state="4. Focus just on what happens around the cutoff."),
  #Step 5: Show the effect
  df %>% mutate(mean_Y = ifelse(xaxisTime > 9 & xaxisTime < 11,mean_Y,NA),Y=ifelse(xaxisTime > 9 & xaxisTime < 11,mean_Y,NA),groupLine=NA,state="5. The jump at the cutoff is the effect of treatment."))
p <- ggplot(dffull,aes(y=Y,x=xaxisTime))+geom_point()+
  geom_vline(aes(xintercept=10),linetype='dashed')+
  geom_point(aes(y=mean_Y,x=groupX),color="red",size=2)+
  geom_vline(aes(xintercept=groupLine))+
  geom_vline(aes(xintercept=cutLine))+
  geom_segment(aes(x=10,xend=10,
                   y=ifelse(state=='5. The jump at the cutoff is the effect of treatment.',
                            filter(df,groupLine==9)$mean_Y[1],NA),
                   yend=filter(df,groupLine==10)$mean_Y[1]),size=1.5,color='blue')+
  scale_color_colorblind()+
  scale_x_continuous(
    breaks = c(5, 15),
    label = c("Untreated", "Treated")
  )+xlab("Running Variable")+
  labs(title = 'The Effect of Treatment on Y using Regression Discontinuity \n{next_state}')+
  transition_states(state,transition_length=c(6,16,6,16,6),state_length=c(50,22,12,22,50),wrap=FALSE)+
  ease_aes('sine-in-out')+
  theme_metro_regtitle() + 
  exit_fade()+enter_fade()
animate(p, nframes=150)
```

---

# RDD: same slope

- The most basic version of RDD allows for a jump but forces the slope to be the **same** on either side
$$Y_i = \beta_0 + \beta_1 Treated_i + \beta_2 XC_i + U_i$$
where 
  - $Treated_i$ is a binary variable equal to $1$ if you are above the cutoff
  - $XC_i$ is the running variable that is centered around the cutoff, i.e. $XC_i = X_i - Cutoff$
  - $\beta_1$ is how the intercept jumps - that is the RDD effect
- Remember that the RDD estimates the average treatment effect among those just **around the cutoff** - LATE

---

# RDD: simulation

- Let us simulate a dataset with the same slope

```{r, echo = TRUE}
set.seed(7)
df <- tibble(X = runif(500),
             Treated = ifelse(X > 0.5, 1, 0),
             XC = X - 0.5,
             Y = 0.7*Treated + XC + rnorm(500, 0, 0.3))
```

```{r, echo = FALSE}
df %>%
  arrange(X) %>%
  slice(c(256:259)) %>%
  knitr::kable()
```

---

# RDD: simulation

```{r, echo = TRUE}
# The true effect is 0.7
m <- lm(Y ~ Treated + XC, df)
```

```{r, echo = FALSE}
msummary(m, stars = TRUE, gof_omit = '^(?!Num)')
```

---

# RDD: varying slopes

- Typically, you will want to let the slope **vary** to either side
- In effect, we are fitting an entirely different regression line on each side of the cutoff
- We can do this by using the following regression

$$Y_i = \beta_0 + \beta_1Treated_i + \beta_2 XC_i + \beta_3Treated_i \times XC_i + U_i$$
where 
  - $\beta_1$ is how the intercept jumps - that's the RDD effect
  - $\beta_3$ is how the slope changes - that's the RKD effect

---

# RDD: simulation

Let us simulate a dataset with the same slope

```{r, echo = TRUE}
set.seed(7)
df <- tibble(X = runif(500),
             Treated = ifelse(X > 0.5, 1, 0),
             XC = X - 0.5,
             Y = 0.7*Treated + XC + 0.5*Treated*XC + rnorm(500, 0, 0.3))
```

```{r, echo = FALSE}
df %>%
  arrange(X) %>%
  slice(c(256:259)) %>%
  knitr::kable()
```

---

# RDD: simulation

```{r, echo = TRUE}
# The true RDD effect is 0.7, and the true RKD effect is 0.5
m <- lm(Y ~ Treated*XC, df)
```

```{r, echo = FALSE}
msummary(m, stars = TRUE, gof_omit = '^(?!Num)')
```

---

# RDD: graphically

- The true model is an RDD effect of $0.7$ with a slope of $1$ to the left of the cutoff and a slope of $1.5$ to the right, so the RKD effect is $0.5$

```{r, echo = FALSE}
ggplot(df, aes(x = XC, y = Y, group = Treated)) + 
  geom_point() + 
  geom_smooth(method = 'lm', color = 'red', se = FALSE, size = 1.5) + 
  geom_vline(aes(xintercept = 0), linetype = 'dashed') + 
  theme_metro() + 
  geom_segment(aes(x = 0, xend = 0, y = -.03, yend = .73), color = 'blue', size = 2) + 
  labs(x = 'Running Variable Centered on Cutoff',
       y = 'Outcome') +
  annotate(geom = 'label', x = 0, y = .73, label = 'RDD Effect', color = 'blue', size = 16/.pt, hjust = 1.05)
```

---

# Choices

- Bandwidth
- Functional form
- Controls

---

# Choices: bandwidth

- The idea of RDD is that people **just around the cutoff** are very much comparable
- So, people far away from the cutoff are not too informative
  - at best they help determine the slope of the fitted lines
- So, we might limit our analysis within just a **narrow window** around the cutoff
- This makes the **exogenous-at-the-jump assumption** more plausible
  - this lets us worry less about functional form over a narrow range, as there is not too much difference between a linear and a square term
  - but it reduces our sample size considerably
- There's a big literature on **optimal bandwidth selection** which balances the addition of bias (from adding people far away from the cutoff) vs. variance (from adding more people so as to improve estimator precision)
- Gelman & Imbens (2019) show that the "naive" RDD estimators place high weights on observations far from the threshold
  - so, it's better to drop these observations

---

# Choices: bandwidth

- Pay attention to the accuracy, standard errors, and sample sizes

```{r, echo = TRUE}
# The true effect is 0.7
m1 <- lm(Y ~ Treated*XC, df)
m2 <- lm(Y ~ Treated*XC, df %>% filter(abs(XC) < 0.25))
m3 <- lm(Y ~ Treated*XC, df %>% filter(abs(XC) < 0.1))
m4 <- lm(Y ~ Treated*XC, df %>% filter(abs(XC) < 0.05))
m5 <- lm(Y ~ Treated*XC, df %>% filter(abs(XC) < 0.01))
```

```{r, echo = FALSE}
msummary(list(m1, m2, m3, m4, m5), stars = TRUE, gof_omit = '^(?!Num)', coef_omit = "Int|XC")
```

---

# Choices: functional form

- Why do we fit only a straight line on either side? 
  - if the true relationship is curvy this will give us the wrong result
- We can be much more flexible, by including polynomials
$$Y_i = \beta_0 + \beta_1Treated_i + \beta_2 XC_i + \beta_3Treated_i \times XC_i$$
$$+ \beta_4 XC_i^2 + \beta_5 Treated_i \times XC_i^2 + U_i$$
where
  - $\beta_1$ remains our jump at the cutoff - the RDD estimate

---

# Choices: functional form

- The interpretation is the same as before - look for the jump
- We want to be careful with polynomials though, and not add too many
  - remember, the more polynomial terms we add, the stranger the behavior of the line at either end of the range of data
  - so, we can get illusory effects generated by having too many terms
- A common approach is to use **non-parametric** regression or **local linear regression**
  - this does not impose any particular shape
  - and it's easy to get a prediction on either side of the cutoff
  - this allows for non-straight lines without dealing with polynomials

---

# Choices: functional form

- Let's look at the same data with a few different functional forms

```{r, echo = TRUE}
set.seed(7)
df <- tibble(X = runif(500),
             Treated = ifelse(X > 0.5, 1, 0),
             XC = X - 0.5,
             Y = 0.7*Treated + XC + 0.6*XC^2 + rnorm(500, 0, 0.3))
```

```{r, echo = FALSE}
df %>%
  arrange(X) %>%
  slice(c(256:259)) %>%
  knitr::kable()
```

---

# Choices: functional form

```{r, echo = TRUE}
# The true effect is 0.7, and the true model is an order-2 polynomial
m <- lm(Y ~ Treated, df)
```

```{r, echo = FALSE}
jump <- coef(m)[2]

ggplot(df, aes(x = XC, y = Y, group = Treated)) + geom_point() + 
  #geom_smooth(method = 'lm', se = FALSE) + 
  geom_line(aes(y = df %>% group_by(Treated) %>% mutate(YM=mean(Y)) %>% pull(YM)),
            color = 'blue') +
  theme_pubr() + 
  labs(x = 'Running Variable Centered on Cutoff',
       y = 'Outcome',
       title = paste0('Simple Above/Below Average. Jump: ', scales::number(jump, accuracy = .001)))
```

---

# Choices: functional form

```{r, echo = TRUE}
# The true effect is 0.7, and the true model is an order-2 polynomial
m <- lm(Y ~ Treated*XC, df)
```

```{r, echo = FALSE}
jump <- coef(m)[2]

ggplot(df, aes(x = XC, y = Y, group = Treated)) + geom_point() + 
  geom_smooth(method = 'lm', se = FALSE) + 
  theme_pubr() + 
  labs(x = 'Running Variable Centered on Cutoff',
       y = 'Outcome',
       title = paste0('Linear RDD. Jump: ', scales::number(jump, accuracy = .001)))
```

---

# Choices: functional form

```{r, echo = TRUE}
# The true effect is 0.7, and the true model is an order-2 polynomial
m <- lm(Y ~ Treated*XC + Treated*I(XC^2), df)
```

```{r, echo = FALSE}
jump <- coef(m)[2]

ggplot(df, aes(x = XC, y = Y, group = Treated)) + geom_point() + 
  geom_smooth(method = 'lm', se = FALSE, formula = y ~ x + I(x^2)) + 
  theme_pubr() + 
  labs(x = 'Running Variable Centered on Cutoff',
       y = 'Outcome',
       title = paste0('Order-2 Polynomial RDD. Jump: ', scales::number(jump, accuracy = .001)))
```

---

# Choices: functional form

```{r, echo = TRUE}
# The true effect is 0.7, and the true model is an order-2 polynomial
m <- lm(Y ~ Treated*XC + Treated*I(XC^2) + Treated*I(XC^3), df)
```

```{r, echo = FALSE}
jump <- coef(m)[2]

ggplot(df, aes(x = XC, y = Y, group = Treated)) + geom_point() + 
  geom_smooth(method = 'lm', se = FALSE, formula = y ~ x + I(x^2) + I(x^3)) + 
  theme_pubr() + 
  labs(x = 'Running Variable Centered on Cutoff',
       y = 'Outcome',
       title = paste0('Order-3 Polynomial RDD. Jump: ', scales::number(jump, accuracy = .001)))
```

---

# Choices: functional form

```{r, echo = TRUE}
m <- lm(Y ~ Treated*XC + Treated*I(XC^2) + Treated*I(XC^3) + Treated*I(XC^4) + 
       Treated*I(XC^5) + Treated*I(XC^6) + Treated*I(XC^7) + Treated*I(XC^8), df)
```

```{r, echo = FALSE}
jump <- coef(m)[2]

ggplot(df, aes(x = XC, y = Y, group = Treated)) + geom_point() + 
  geom_smooth(method = 'lm', se = FALSE, formula = y ~ poly(x,8)) + 
  theme_pubr() + 
  labs(x = 'Running Variable Centered on Cutoff',
       y = 'Outcome',
       title = paste0('Order-8 Polynomial RDD. Jump: ', scales::number(jump, accuracy = .001)))
```

---

# Choices: functional form

```{r, echo = TRUE}
# The true effect is 0.7, and the true model is an order-2 polynomial
# The estimated model is recommended by Gelman & Imbens (2019)
```

```{r, echo = FALSE}
df <- df %>%
  arrange(XC)
m1 <- loess(Y ~ XC, df %>% filter(Treated == 0))
m2 <- loess(Y ~ XC, df %>% filter(Treated == 1))
jump <- predict(m2)[1] - utils::tail(predict(m1),1)

ggplot(df, aes(x = XC, y = Y, group = Treated)) + geom_point() + 
  geom_smooth(method = 'loess', se = FALSE) + 
  theme_pubr() + 
  labs(x = 'Running Variable Centered on Cutoff',
       y = 'Outcome',
       title = paste0('Local Linear Regression RDD. Jump: ', scales::number(jump, accuracy = .001)))
```

---

# Choices: functional form

- A conclusion is to **avoid** higher-order polynomials
  - even the true model can be worse than something simpler sometimes
  - and fewer terms makes more sense too, once we apply a bandwidth and zoom in
  - consider a nonparametric approach
- Gelman & Imbens (2019) argue that controlling for global high-order polynomials in RDD has three major problems
  - noisy estimates
  - sensitivity to the degree of the polynomial
  - poor coverage of confidence intervals
- Be very suspicious if your fit is wildly off right around the cutoff

---

# Choices: controls

- Generally, you don't need control variables in an RDD
  - if the design is valid, RDD is almost like a randomized experiment
- Although maybe we want some controls if we a bandwidth is **wide** 
  - this will remove some of the bias
- Control variables also allow us to perform **placebo tests** of our RDD model
  - we can rerun our RDD model, but simply use a **control** variable as the **outcome**
  - we should not find any effect
  - you can run these for every control variable you have

---

# Assumptions

- We knew there must be some assumptions
  - some are more obvious, i.e. we should be using the correct functional form
  - others are trickier, i.e. what are we assuming about the error term and endogeneity?
- Specifically, we are assuming that the only thing jumping at the cutoff is **treatment**
  - sort of like parallel trends, but maybe more believable since we've narrowed in 
- For example, if having an income below $150\%$ of the poverty line gets you access to food stamps **and** to job training, then we can't really use that cutoff to get the effect of **just** food stamps
- The only thing different about just above/just below should be treatment
  - but what if the running variable is **manipulated**?

---

# Assumptions: bunching

- Imagine you are a teacher grading the gifted-and-talented exam. You see someone with an $74$ and think "they are so close, I'll just give them an extra point"
  - suddenly, that treatment is a lot less randomly assigned around the cutoff
- If there's manipulation of the running variable around the cutoff, we can often see it in the presence of **bunching**
  - in other words, there is a big **cluster of observations** to one side of the cutoff and a seeming gap missing on the other side
- How can we check this?
  - we can look graphically by just checking for a jump at the cutoff in **number of observations** after binning
  - we can use the **McCrary density test** in `rddensity` package

---

# Assumptions: bunching

- The first one looks pretty good. The second one looks not-so-good

```{r, echo = FALSE}
df_bin_count <- df %>%
  # Select breaks so that one of the breakpoints is the cutoff
  mutate(X_bins = cut(X, breaks = 0:10/10)) %>%
  group_by(X_bins) %>%
  count()

bad_bins <- df_bin_count 
bad_bins$n <- sample(df_bin_count$n, 10)
bad_bins$n[5] <- 20
bad_bins$n[6] <- 100
bad_bins$Type <- 'Bad'
df_bin_count %>%
  mutate(Type = 'Good') %>%
  bind_rows(bad_bins) %>%
  mutate(Type = factor(Type, levels = c('Good','Bad'))) %>%
  group_by(Type) %>%
  mutate(n = n/sum(n)) %>%
  ggplot(aes(x = X_bins, y = n, fill = Type)) + 
  guides(fill = FALSE) + 
  geom_col() + 
  theme_metro() +
  theme(axis.text.x = element_text(angle = 90)) + 
  labs(y = '%', x = "X") + 
  geom_vline(aes(xintercept = 5.5), linetype = 'dashed') +
  scale_y_continuous(labels = scales::percent, limits = c(0,.2)) +
  facet_wrap('Type')
```

---

# Assumptions: bunching

```{r, echo = TRUE}
library(rddensity)
test_density <- rddensity(df$X, c = 0.5)
```

- The p-value of a t-test $0.8959$ shows no manipulation

```{r, echo = FALSE}
summary(test_density)
```

---

# Assumptions: bunching

```{r, echo = TRUE, fig.width = 4.5, fig.height = 2.2}
plot_density_test <- rdplotdensity(rdd = test_density, X = df$X)
```

- Notice that the confidence intervals overlap substantially

---

# Fuzzy RDD

- What if treatment is not determined sharply by the cutoff?
  - we can account for this with a model designed to take this into account
- Specifically, we can use the IV method
  - basically, IV estimates how much the **chances of treatment** go up at the cutoff, and scales the **estimate of treatment** by that change (remember $4^{th}$ TA about LATE)
  - we can perform the IV method using `feols()` in `fixest`
- What happens if we just do RDD as normal? 
  - the effect is **underestimated** because we have some untreated in the post-cutoff and treated in the pre-cutoff

---

# Fuzzy RDD: simulation

- Let us simulate a dataset with imperfect compliance

```{r, echo = TRUE}
set.seed(77)
df_fuzzy <- tibble(X = runif(500),
                   above_cut = ifelse(X > 0.5, 1, 0),
                   XC = X - 0.5,
                   treatassign = 0.5*XC + 0.5*above_cut,
                   random = runif(500),
                   Treated = ifelse(treatassign > random, 1, 0),
                   Y = 0.7*Treated + XC + rnorm(500, 0, 0.3))
```

```{r, echo = FALSE}
df_fuzzy %>%
  arrange(X) %>%
  slice(c(266:269)) %>%
  knitr::kable()
```

---

# Fuzzy RDD: simulation

- Notice that the y-axis here is not the outcome, it is the percentage treated

```{r, echo = FALSE}
df_fuzzy <- df_fuzzy %>%
  mutate(Runbin = cut(X, 0:10/10)) %>%
  group_by(Runbin) %>%
  mutate(av_treat = mean(treatassign),
         av_out = mean(Y))
ggplot(df_fuzzy, aes(x = X, y = treatassign)) + 
  geom_point() + 
  geom_point(data = df_fuzzy %>% group_by(Runbin) %>% slice(1), aes(x = X, y = av_treat),
             color = 'red', size = 2) +
  geom_smooth(aes(group = X > .5), method = 'lm', color = 'blue', se = FALSE) +
  geom_vline(aes(xintercept = .5), linetype = 'dashed') + 
  ggpubr::theme_pubr() + 
  labs(x = 'Running Variable', y = 'Treated')
```

---

# Fuzzy RDD: simulation

```{r, echo = TRUE}
# The true effect is 0.7
without_fuzzy <- lm(Y ~ above_cut*XC, df_fuzzy)
predict_treat <- lm(Treated ~ above_cut*XC, df_fuzzy)
fuzzy_rdd     <- feols(Y ~ 1 | Treated*XC ~ above_cut*XC, df_fuzzy)
```

```{r, echo = FALSE}
msummary(list(without_fuzzy, predict_treat, fuzzy_rdd), stars = TRUE, gof_omit = '^(?!Num)', coef_omit = "Int|XC")
```

---

# RDD: estimation

- The `rdrobust` package has the `rdrobust()` function which runs RDD with
  - optimal bandwidth selection
  - options for fuzzy RD
  - bias correction
  - lots of options (including the addition of covariates)

```{r, echo = TRUE}
# The true effect is 0.7
library(rdrobust)
m <- rdrobust(df$Y, df$X, c = 0.5)
```

- We can estimate the RDD model by specifying
  - `Y` - a dependent variable
  - `X` - a running variable
  - `c` - a cutoff
  - `p` - the number of polynomials (it applies polynomials more locally than our OLS models do - it avoids weird corner predictions)
  - `h` - a bandwidth size (chosen automatically) 
  - `fuzzy` - actual treatment outside of the running variable/cutoff combo (IV)

---

# RDD: estimation

```{r, echo = FALSE}
summary(m)
```

---

# RDD: estimation

- A previous model chose the bandwidth of $0.212$
- A common approach to sensitivity analysis is to use 
  - the ideal bandwidth
  - twice the ideal
  - half the ideal
  - and see if the estimate changes substantially

```{r, echo = TRUE}
# The true effect is 0.7
library(rdrobust)
m1 <- rdrobust(df$Y, df$X, c = 0.5, h = 0.212)
m2 <- rdrobust(df$Y, df$X, c = 0.5, h = 2*0.212)
m3 <- rdrobust(df$Y, df$X, c = 0.5, h = 0.5*0.212)
```

```{r, echo = FALSE}
c(m1[["coef"]][[1]], m2[["coef"]][[1]], m3[["coef"]][[1]])
```

---

# RDD: estimation

- Now plot the results
- Note that `rdplot()` uses order-4 polynomial, and `rdrobust()` - local linear regression

```{r, echo = TRUE, dpi = 150, fig.width = 5, fig.height = 2.8}
rdplot(df$Y, df$X, c = 0.5)
```

---

# References

Books
- Huntington-Klein, N. The Effect: An Introduction to Research Design and Causality, [Chapter 20: Regression Discontinuity](https://theeffectbook.net/ch-RegressionDiscontinuity.html)
- Cunningham, S. Causal Inference: The Mixtape, [Chapter 6: Regression Discontinuity](https://mixtape.scunning.com/regression-discontinuity.html)

Slides
- Huntington-Klein, N. Econometrics Course, [Week 08: Regression Discontinuity](https://github.com/NickCH-K/EconometricsSlides/blob/master/Week_08/Week_08_Regression_Discontinuity.html)
- Huntington-Klein, N. Causality Inference Course, [Lecture 12: Regression Discontinuity](https://github.com/NickCH-K/CausalitySlides/blob/main/Lecture_12_Regression_Discontinuity.html)
- Huntington-Klein, N. Causality Inference Course, [Lecture 13: Estimating Regression Discontinuity](https://github.com/NickCH-K/CausalitySlides/blob/main/Lecture_13_Estimating_Regression_Discontinuity.html)

Articles
- Gelman, A., & Imbens, G. (2019). [Why High-Order Polynomials Should not be Used in Regression Discontinuity Designs](https://www.tandfonline.com/doi/abs/10.1080/07350015.2017.1366909). Journal of Business & Economic Statistics, 37(3), 447-456.
